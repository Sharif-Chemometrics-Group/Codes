{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ef55ed",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "\n",
    "gc-ims-tools: Custom package to handle and analyze GC-IMS data\n",
    "\n",
    "Pillow: For image handling (replacement for PIL)\n",
    "\n",
    "OpenCV: For image processing\n",
    "\n",
    "tensorflow: Deep learning framework (model, training, saliency maps)\n",
    "\n",
    "tf-keras-vis: For generating saliency maps (e.g., SmoothGrad, etc.)\n",
    "\n",
    "scikit-learn: For KFold, PCA, classification report, confusion matrix\n",
    "\n",
    "scikit-image: For peak detection\n",
    "\n",
    "numpy: For numerical array and matrix operations\n",
    "\n",
    "matplotlib: For plotting images and graphs\n",
    "\n",
    "seaborn: For enhanced plotting (e.g., confusion matrix heatmap)\n",
    "\n",
    "tqdm: For progress bars in loops\n",
    "\n",
    "Install all packages using pip:\n",
    "\n",
    "pip install gc-ims-tools tensorflow tf-keras-vis scikit-learn scikit-image matplotlib seaborn tqdm Pillow opencv-python\n",
    "\n",
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56500a",
   "metadata": {},
   "source": [
    "The following script performs preprocessing of GC-IMS data for use in machine learning applications, such as CNN-based classification.\n",
    "The workflow includes data import, optional resolution reduction through binning, alignment of chromatograms using the\n",
    "Reactant Ion Peak (RIP), cropping of non-informative regions along both drift time and retention time axes,\n",
    "and exporting the resulting chromatograms as 2D `.npy` files in the class subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b45f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the IMS package for processing GC-IMS data\n",
    "import ims\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the dataset from the specified directory. The dataset contains .mea files,\n",
    "ds = ims.Dataset.read_mea(\n",
    "    r\"Dataset directory\",\n",
    "    subfolders=True,\n",
    ")\n",
    "print(\"Data Import Completed\")\n",
    "\n",
    "# Optional: Apply binning with a factor of 2 (reduce computational cost for system with limited RAM)\n",
    "ds_bin = ds.binning(2)\n",
    "print(\"Binning complete\")\n",
    "\n",
    "# Align chromatograms based on the Reactant Ion Peak (RIP) using relative interpolation.\n",
    "# This step is very memory intensive.\n",
    "ds_bin_rip = ds.interp_riprel()\n",
    "print(\"RIP complete\")\n",
    "\n",
    "# Crop the chromatograms to remove non-informative or noisy regions:\n",
    "# For Example:\n",
    "# - Drift time is restricted to the range of 1.05 to 2.5 ms relative to RIP.\n",
    "# - Retention time is limited to the range of 50 to 900 seconds.\n",
    "ds_cut = ds_bin_rip.cut_dt(1.05, 2.5).cut_rt(50, 900)\n",
    "print(\"Cut complete\")\n",
    "\n",
    "X, Y = ds_cut.get_xy(flatten=False)\n",
    "\n",
    "base_output_dir = Path(r\"Output directory\")\n",
    "base_output_dir.mkdir()\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "for sample_2d, label in zip(X, Y):\n",
    "    label_str = str(label)\n",
    "    label_counts[label_str] += 1\n",
    "    current_count = label_counts[label_str]\n",
    "    label_folder_path = base_output_dir / label_str\n",
    "    label_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_name = f\"{label_str}_{current_count}.npy\"\n",
    "    file_path = label_folder_path / file_name\n",
    "\n",
    "    # Save the 2D sample array\n",
    "    np.save(file_path, sample_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc8ccf",
   "metadata": {},
   "source": [
    "This script implements a domain-specific data augmentation technique for GC-IMS chromatographic data, where peak regions are detected and selectively shifted vertically to simulate retention time variability The augmentation preserves chemical relevance by identifying peak locations using local maxima detection, \n",
    "then applying controlled directional shifts within predefined regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e97cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import peak_local_max\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def augment_npy(npy_path: str, output_path: str, direction: str):\n",
    "\n",
    "    # Parameters\n",
    "    Min_Peak_Distance = 15\n",
    "    Peak_Threshold = 0.2\n",
    "    Peak_Box_Size = 30\n",
    "    Max_Vertical_Shift = 8\n",
    "    Peak_Shift_Fraction = 0.5\n",
    "\n",
    "    try:\n",
    "        data_2d = np.load(npy_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read .npy file {npy_path}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    if data_2d.ndim != 2:\n",
    "        print(\n",
    "            f\"Skipping {npy_path}: Data is not 2D (shape: {data_2d.shape}). Copying original.\"\n",
    "        )\n",
    "        np.save(output_path, data_2d)\n",
    "        return True\n",
    "\n",
    "    # Peak detection\n",
    "    min_val = data_2d.min()\n",
    "    max_val = data_2d.max()\n",
    "\n",
    "    data_normalized = np.zeros_like(data_2d, dtype=np.float32)\n",
    "    if (max_val - min_val) > 1e-6:\n",
    "        data_normalized = (data_2d - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Find peaks on the normalized data\n",
    "    peak_coords = peak_local_max(\n",
    "        data_normalized, min_distance=Min_Peak_Distance, threshold_abs=Peak_Threshold\n",
    "    )\n",
    "\n",
    "    if len(peak_coords) == 0:\n",
    "        np.save(output_path, data_2d)\n",
    "        return True\n",
    "\n",
    "    # Augmentation\n",
    "    augmented_data = np.copy(data_2d)\n",
    "\n",
    "    # Select a fraction of peaks to shift\n",
    "    num_to_shift = int(len(peak_coords) * Peak_Shift_Fraction)\n",
    "    if num_to_shift == 0 and len(peak_coords) > 0:\n",
    "        num_to_shift = 1\n",
    "\n",
    "    indices_to_shift = np.random.choice(\n",
    "        len(peak_coords), size=num_to_shift, replace=False\n",
    "    )\n",
    "\n",
    "    for i in indices_to_shift:\n",
    "        y_peak, x_peak = peak_coords[i]\n",
    "\n",
    "        # Determine shift direction\n",
    "        if direction == \"up\":\n",
    "            shift = random.randint(-Max_Vertical_Shift, -1)  # Must be negative\n",
    "        elif direction == \"down\":\n",
    "            shift = random.randint(1, Max_Vertical_Shift)  # Must be positive\n",
    "        else:  # 'both'\n",
    "            shift = random.randint(-Max_Vertical_Shift, Max_Vertical_Shift)\n",
    "\n",
    "        if shift == 0:\n",
    "            continue\n",
    "\n",
    "        # Define peak box boundaries\n",
    "        half_box = Peak_Box_Size // 2\n",
    "        y_start, y_end = y_peak - half_box, y_peak + half_box\n",
    "        x_start, x_end = x_peak - half_box, x_peak + half_box\n",
    "        new_y_start, new_y_end = y_start + shift, y_end + shift\n",
    "\n",
    "        # Boundary checks (ensure box is within array)\n",
    "        if (\n",
    "            y_start < 0\n",
    "            or y_end > data_2d.shape[0]\n",
    "            or x_start < 0\n",
    "            or x_end > data_2d.shape[1]\n",
    "            or new_y_start < 0\n",
    "            or new_y_end > data_2d.shape[0]\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Copy the original peak data before modifying the array\n",
    "        peak_roi = augmented_data[y_start:y_end, x_start:x_end].copy()\n",
    "\n",
    "        # Padding\n",
    "        # Fill the \"hole\" left by the peak with the adjacent row of pixels\n",
    "        if shift > 0:  # Moved DOWN\n",
    "            pad_source_y = max(0, y_start - 1)\n",
    "            pad_row = augmented_data[pad_source_y, x_start:x_end]\n",
    "            for y in range(y_start, y_end):\n",
    "                augmented_data[y, x_start:x_end] = pad_row\n",
    "        else:  # Moved UP\n",
    "            pad_source_y = min(data_2d.shape[0] - 1, y_end)\n",
    "            pad_row = augmented_data[pad_source_y, x_start:x_end]\n",
    "            for y in range(y_start, y_end):\n",
    "                augmented_data[y, x_start:x_end] = pad_row\n",
    "\n",
    "        # Place the original peak into the new, shifted location\n",
    "        augmented_data[new_y_start:new_y_end, x_start:x_end] = peak_roi\n",
    "\n",
    "    # Save the final augmented array as a .npy file\n",
    "    np.save(output_path, augmented_data)\n",
    "    return True\n",
    "\n",
    "def process_dataset_npy(\n",
    "    input_dir: str, output_dir: str, num_augmentations_per_file: int, direction: str\n",
    "):\n",
    "    \n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    print(f\"Input directory: {input_path}\")\n",
    "    print(f\"Output directory: {output_path}\")\n",
    "\n",
    "    # Clean up output directory\n",
    "    if output_path.exists():\n",
    "        print(f\"Removing old output directory: {output_path}\")\n",
    "        shutil.rmtree(output_path)\n",
    "    output_path.mkdir(parents=True)\n",
    "\n",
    "    # Search for .npy files instead of images\n",
    "    npy_files = list(input_path.glob(\"**/*.npy\"))\n",
    "\n",
    "    if not npy_files:\n",
    "        print(f\"Error: No .npy files found in the input directory '{input_dir}'.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(npy_files)\n",
    "    print(f\"Found {total_files} .npy files to process.\")\n",
    "\n",
    "    for i, npy_file in enumerate(npy_files):\n",
    "        # Create the same subfolder structure in the output directory\n",
    "        relative_path = npy_file.relative_to(input_path)\n",
    "        output_original_path = output_path / relative_path\n",
    "\n",
    "        output_original_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Copy the original file\n",
    "        shutil.copy(npy_file, output_original_path)\n",
    "\n",
    "        # Create augmented versions\n",
    "        for n in range(num_augmentations_per_file):\n",
    "            aug_stem = output_original_path.stem + f\"_aug_{n + 1}\"\n",
    "            aug_suffix = output_original_path.suffix\n",
    "            output_augmented_path = output_original_path.with_name(\n",
    "                f\"{aug_stem}{aug_suffix}\"\n",
    "            )\n",
    "\n",
    "            augment_npy(str(npy_file), str(output_augmented_path), direction)\n",
    "\n",
    "        print(\n",
    "            f\"({i+1}/{total_files}) Processed '{npy_file.name}' -> saved original + {num_augmentations_per_file} augmented versions.\"\n",
    "        )\n",
    "\n",
    "    total_original = total_files\n",
    "    total_augmented = total_files * num_augmentations_per_file\n",
    "    grand_total = total_original + total_augmented\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(\n",
    "        f\"Successfully created a new dataset with {grand_total} files in '{output_dir}'.\"\n",
    "    )\n",
    "    print(f\"({total_original} original + {total_augmented} augmented)\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# Number of augmented versions for each original file\n",
    "Num_Augmentations_Per_File = 2\n",
    "\n",
    "# Set the direction for the peak shift\n",
    "Shift_Direction = \"up\"  # Options: 'up', 'down', 'both'\n",
    "\n",
    "# Path to original .npy dataset\n",
    "Input_Directory = r\"Path for npy files\"\n",
    "\n",
    "# Path to new augmented dataset\n",
    "Output_Directory = r\"Output directory for augmented data\"\n",
    "\n",
    "# Run the processing\n",
    "process_dataset_npy(\n",
    "    Input_Directory,\n",
    "    Output_Directory,\n",
    "    Num_Augmentations_Per_File,\n",
    "    Shift_Direction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e399f7",
   "metadata": {},
   "source": [
    "The following script implements a CNN-based classification pipeline using k-fold cross-validation to evaluate the model's generalization performance across GC-IMS data.\n",
    "The pipeline includes model definition, data loading, cross-validated training, per-fold performance visualization, and statistical reporting of classification metrics with standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Define the CNN model architecture\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            8, (3, 3), strides=(1, 1), padding=\"same\", input_shape=input_shape\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(48, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = (248, 200, 3)\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "# Path to dataset directory containing class subfolders\n",
    "data_dir = r\"Directory for .npy files\"\n",
    "\n",
    "# Load all .npy files and labels into memory manually\n",
    "X_list = []\n",
    "y_list = []\n",
    "class_indices = {}\n",
    "current_label_index = 0\n",
    "\n",
    "data_path = Path(data_dir)\n",
    "if not data_path.exists():\n",
    "    print(f\"Error: Data directory not found at {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "sorted_class_dirs = sorted([d for d in data_path.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"Loading .npy files from {data_dir}...\")\n",
    "for class_dir in sorted_class_dirs:\n",
    "    class_name = class_dir.name\n",
    "    if class_name not in class_indices:\n",
    "        class_indices[class_name] = current_label_index\n",
    "        current_label_index += 1\n",
    "    label = class_indices[class_name]\n",
    "    for npy_file in class_dir.glob(\"*.npy\"):\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "\n",
    "            # Convert to tensor, add channel, resize, and convert back to numpy\n",
    "            data_tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "            data_expanded = tf.expand_dims(data_tensor, axis=-1)\n",
    "            data_resized_tensor = tf.image.resize(\n",
    "                data_expanded, (input_shape[0], input_shape[1])\n",
    "            )\n",
    "            data_resized_numpy = data_resized_tensor.numpy()\n",
    "\n",
    "            # Perform per-sample Min-Max scaling to [0, 1]\n",
    "            max_val = data_resized_numpy.max()\n",
    "            data_scaled = data_resized_numpy / max_val\n",
    "\n",
    "            X_list.append(data_scaled)\n",
    "            y_list.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing {npy_file}: {e}\")\n",
    "\n",
    "if not X_list:\n",
    "    print(f\"Error: No .npy files were successfully loaded from {data_dir}.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X_list)\n",
    "y_labels_int = np.array(y_list)\n",
    "\n",
    "# Convert labels to one-hot encoded\n",
    "Y = tf.keras.utils.to_categorical(y_labels_int, num_classes=num_classes)\n",
    "\n",
    "class_names = list(class_indices.keys())\n",
    "print(f\"Loading complete. Found {len(X)} samples.\")\n",
    "print(f\"Data shape X original: {data_expanded.shape}\")\n",
    "print(f\"Data shape X: {X.shape}, Y: {Y.shape}\")\n",
    "print(f\"Class mapping: {class_indices}\")\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "histories = []\n",
    "\n",
    "# Dictionary for storing precision, recall, and F1-score per class per fold\n",
    "test_metrics_per_class = defaultdict(list)\n",
    "\n",
    "# Begin k-fold training and evaluation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, Y)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "\n",
    "    # Build and compile a new model instance for this fold\n",
    "    model = create_cnn_model(input_shape, num_classes)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "    # Train the model and store training history\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "    histories.append(history)\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    scores = model.evaluate(X_val, Y_val)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    print(\n",
    "        f\"Score for fold {fold + 1}: Loss = {scores[0]:.4f}; Accuracy = {scores[1] * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Plot accuracy and loss for this fold\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\", linestyle=\"--\")\n",
    "    plt.title(f\"Fold {fold + 1} Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\", linestyle=\"--\")\n",
    "    plt.title(f\"Fold {fold + 1} Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"accuracy_loss_fold_{fold + 1}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save trained model for this fold\n",
    "    model.save(f\"cnn_model_fold_{fold + 1}.h5\")\n",
    "    print(f\"Model for fold {fold + 1} saved.\")\n",
    "\n",
    "    # Generate and store classification metrics\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "    y_true = np.argmax(Y_val, axis=1)\n",
    "    y_pred = np.argmax(Y_val_pred, axis=1)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    for cls in report:\n",
    "        if cls in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "            continue\n",
    "        for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            test_metrics_per_class[f\"{cls}_{metric}\"].append(report[cls][metric])\n",
    "\n",
    "# Plot average accuracy and loss across folds\n",
    "average_train_accuracy = np.mean([h.history[\"accuracy\"] for h in histories], axis=0)\n",
    "average_val_accuracy = np.mean([h.history[\"val_accuracy\"] for h in histories], axis=0)\n",
    "average_loss = np.mean([h.history[\"loss\"] for h in histories], axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(average_train_accuracy, label=\"Avg Train Accuracy\", color=\"green\")\n",
    "plt.plot(average_val_accuracy, label=\"Avg Val Accuracy\", linestyle=\"--\", color=\"blue\")\n",
    "plt.title(\"Average Accuracy Across Folds\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(average_loss, label=\"Avg Loss\", color=\"orange\")\n",
    "plt.title(\"Average Loss Across Folds\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_accuracy_loss_across_folds.png\")\n",
    "plt.close()\n",
    "\n",
    "# Print overall performance summary\n",
    "print(\"Average scores across all folds:\")\n",
    "print(f\"> Accuracy: {np.mean(acc_per_fold):.2f}% (± {np.std(acc_per_fold):.2f}%)\")\n",
    "print(f\"> Loss: {np.mean(loss_per_fold):.4f}\")\n",
    "\n",
    "# Display class-wise metrics with standard deviation\n",
    "print(\"\\nFinal Classification Report with Standard Deviation Across Folds:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"\\nClass '{cls}' (Label {i}):\")\n",
    "    for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "        values = test_metrics_per_class[f\"{i}_{metric}\"]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"  {metric}: {mean_val:.4f} ± {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e37fed",
   "metadata": {},
   "source": [
    "The following script implements a CNN-based classification pipeline for GC-IMS data with repeated random train-test splits to assess the model's performance robustness. \n",
    "The pipeline includes model definition, image loading, training across multiple random splits, per-run evaluation and visualization, and statistical reporting of classification metrics (precision, recall, F1-score) with mean and standard deviation across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "# Define the CNN model architecture\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            8, (3, 3), strides=(1, 1), padding=\"same\", input_shape=input_shape\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(48, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = (248, 200, 1)\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "# Path to dataset directory containing class subfolders\n",
    "data_dir = r\"Directory for .npy files\"\n",
    "\n",
    "# Load all .npy files and labels into memory manually\n",
    "X_list = []\n",
    "y_list = []\n",
    "class_indices = {}\n",
    "current_label_index = 0\n",
    "\n",
    "data_path = Path(data_dir)\n",
    "if not data_path.exists():\n",
    "    print(f\"Error: Data directory not found at {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "sorted_class_dirs = sorted([d for d in data_path.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"Loading .npy files from {data_dir}...\")\n",
    "for class_dir in sorted_class_dirs:\n",
    "    class_name = class_dir.name\n",
    "    if class_name not in class_indices:\n",
    "        class_indices[class_name] = current_label_index\n",
    "        current_label_index += 1\n",
    "\n",
    "    label = class_indices[class_name]\n",
    "    for npy_file in class_dir.glob(\"*.npy\"):\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Convert to tensor, add channel, resize, and convert back to numpy\n",
    "            data_tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "            data_expanded = tf.expand_dims(data_tensor, axis=-1)\n",
    "            data_resized_tensor = tf.image.resize(\n",
    "                data_expanded, (input_shape[0], input_shape[1])\n",
    "            )\n",
    "            data_resized_numpy = data_resized_tensor.numpy()\n",
    "\n",
    "            # Perform per-sample Min-Max scaling to [0, 1]\n",
    "            max_val = data_resized_numpy.max()\n",
    "            data_scaled = data_resized_numpy / max_val\n",
    "\n",
    "            X_list.append(data_scaled)\n",
    "            y_list.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing {npy_file}: {e}\")\n",
    "\n",
    "if not X_list:\n",
    "    print(f\"Error: No .npy files were successfully loaded from {data_dir}.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X_list)\n",
    "y_labels_int = np.array(y_list)\n",
    "\n",
    "# Convert integer labels to one-hot encoded\n",
    "Y = tf.keras.utils.to_categorical(y_labels_int, num_classes=num_classes)\n",
    "\n",
    "class_names = list(class_indices.keys())\n",
    "print(f\"Loading complete. Found {len(X)} samples.\")\n",
    "print(f\"Data shape X original: {data_expanded.shape}\")\n",
    "print(f\"Data shape X: {X.shape}, Y: {Y.shape}\")\n",
    "print(f\"Class mapping: {class_indices}\")\n",
    "\n",
    "# Number of repeated random train/test runs\n",
    "n_runs = 5\n",
    "overall_accuracies = []\n",
    "overall_losses = []\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# Repeat training/testing over multiple random splits\n",
    "for run in range(n_runs):\n",
    "\n",
    "    print(f\"\\nRun {run+1}/{n_runs} on random split:\")\n",
    "\n",
    "    # Create and compile the CNN model\n",
    "    cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "    cnn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Split data randomly into 80% train and 20% test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Train the model and store training history\n",
    "    history = cnn_model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, Y_test),\n",
    "    )\n",
    "\n",
    "    # Evaluate model on the test set\n",
    "    test_scores = cnn_model.evaluate(X_test, Y_test)\n",
    "    print(f\"Test loss: {test_scores[0]}\")\n",
    "    print(f\"Test accuracy: {test_scores[1]*100}%\")\n",
    "\n",
    "    # Save test accuracy and loss for this run\n",
    "    overall_accuracies.append(test_scores[1])\n",
    "    overall_losses.append(test_scores[0])\n",
    "\n",
    "    # Predict on test set\n",
    "    Y_pred = cnn_model.predict(X_test)\n",
    "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "    Y_true_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Store predictions and ground truths for cumulative confusion matrix\n",
    "    all_true_labels.extend(Y_true_classes)\n",
    "    all_pred_labels.extend(Y_pred_classes)\n",
    "\n",
    "    # Generate and print classification report\n",
    "    print(f\"Classification Report for Run {run+1}:\")\n",
    "    report = classification_report(Y_true_classes, Y_pred_classes, output_dict=True)\n",
    "    print(report)\n",
    "\n",
    "    # Extract and store precision, recall, and F1-score per class\n",
    "    precision_list.append([report[str(i)][\"precision\"] for i in range(num_classes)])\n",
    "    recall_list.append([report[str(i)][\"recall\"] for i in range(num_classes)])\n",
    "    f1_score_list.append([report[str(i)][\"f1-score\"] for i in range(num_classes)])\n",
    "\n",
    "    # Confusion matrix for this run\n",
    "    cm = confusion_matrix(Y_true_classes, Y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix for Run {run+1}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrix_run_{run+1}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation accuracy/loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(\n",
    "        history.history[\"val_accuracy\"], label=\"Validation Accuracy\", linestyle=\"--\"\n",
    "    )\n",
    "    plt.title(f\"Run {run+1} - Model Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", linestyle=\"--\")\n",
    "    plt.title(f\"Run {run+1} - Model Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"accuracy_loss_plot_run_{run+1}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Print mean accuracy and loss across runs\n",
    "print(\"\\nSummary of results from all runs:\")\n",
    "print(f\"Average accuracy: {np.mean(overall_accuracies)*100:.2f}%\")\n",
    "print(f\"Average loss: {np.mean(overall_losses):.4f}\")\n",
    "\n",
    "# Compute average and standard deviation of metrics\n",
    "precision_mean = np.mean(precision_list, axis=0)\n",
    "precision_std = np.std(precision_list, axis=0)\n",
    "recall_mean = np.mean(recall_list, axis=0)\n",
    "recall_std = np.std(recall_list, axis=0)\n",
    "f1_score_mean = np.mean(f1_score_list, axis=0)\n",
    "f1_score_std = np.std(f1_score_list, axis=0)\n",
    "\n",
    "# Display final classification report with error bars\n",
    "print(\"\\nOverall Classification Report (Mean ± Standard Deviation):\")\n",
    "print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(\n",
    "        f\"{class_name}\\t{precision_mean[i]:.2f} ± {precision_std[i]:.2f}\\t{recall_mean[i]:.2f} ± {recall_std[i]:.2f}\\t{f1_score_mean[i]:.2f} ± {f1_score_std[i]:.2f}\"\n",
    "    )\n",
    "\n",
    "# Create and save the final cumulative confusion matrix\n",
    "overall_cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    overall_cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"overall_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5759a",
   "metadata": {},
   "source": [
    "The following script performs inference using a pre-trained CNN model on GC-IMS heatmap images.\n",
    "The pipeline includes model loading, preprocessing of test images using consistent normalization, generating predictions, mapping predicted indices to class labels, and saving the results to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd624a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Define image parameters and model path\n",
    "input_size = (248, 200)\n",
    "input_shape = (248, 200, 1)  # Shape expected by the model (H, W, Channels)\n",
    "model_path = r\"Path to the trained CNN model\"\n",
    "\n",
    "# Load the trained model from file\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Successfully loaded model from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {model_path}. Error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Set the path to the test image directory\n",
    "test_data_dir = r\"Directory containing test data\"\n",
    "\n",
    "# Manually load, resize, and scale all .npy files\n",
    "X_list = []\n",
    "filenames_list = []\n",
    "class_indices = {}\n",
    "\n",
    "data_path = Path(test_data_dir)\n",
    "if not data_path.exists():\n",
    "    print(f\"Error: Data directory not found at {test_data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Loading .npy files from {test_data_dir}...\")\n",
    "npy_files = sorted(list(data_path.glob(\"**/*.npy\")))\n",
    "\n",
    "if not npy_files:\n",
    "    print(f\"Error: No .npy files found in {test_data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Manually build class_indices from parent folder names\n",
    "class_names_set = set()\n",
    "for npy_file in npy_files:\n",
    "    class_name = npy_file.parent.name\n",
    "    class_names_set.add(class_name)\n",
    "\n",
    "sorted_class_names = sorted(list(class_names_set))\n",
    "class_indices = {name: i for i, name in enumerate(sorted_class_names)}\n",
    "print(f\"Found classes: {class_indices}\")\n",
    "\n",
    "# Now process the files\n",
    "for npy_file in npy_files:\n",
    "    try:\n",
    "        relative_path_str = str(npy_file.relative_to(data_path))\n",
    "        if os.sep != \"/\":\n",
    "            relative_path_str = relative_path_str.replace(os.sep, \"/\")\n",
    "        filenames_list.append(relative_path_str)\n",
    "\n",
    "        # Load, resize, and scale data (same as training scripts)\n",
    "        data = np.load(npy_file)\n",
    "\n",
    "        data_tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "        data_expanded = tf.expand_dims(data_tensor, axis=-1)\n",
    "        data_resized_tensor = tf.image.resize(\n",
    "            data_expanded, (input_shape[0], input_shape[1])\n",
    "        )\n",
    "        data_resized_numpy = data_resized_tensor.numpy()\n",
    "\n",
    "        max_val = data_resized_numpy.max()\n",
    "        data_scaled = data_resized_numpy / max_val \n",
    "\n",
    "        X_list.append(data_scaled)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {npy_file}: {e}\")\n",
    "        filenames_list.pop()\n",
    "\n",
    "# Convert list to a single NumPy array\n",
    "X_test = np.array(X_list)\n",
    "\n",
    "if len(X_test) == 0:\n",
    "    print(\"No data was successfully loaded. Exiting.\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Loading complete. Found {len(X_test)} samples.\")\n",
    "print(f\"Data shape X_test: {X_test.shape}\")\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map class indices to actual class labels\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "predicted_labels = [idx_to_class[i] for i in predicted_classes]\n",
    "\n",
    "# Retrieve filenames corresponding to the predictions\n",
    "filenames = filenames_list\n",
    "\n",
    "# Display predictions alongside corresponding filenames\n",
    "for fname, pred_label in zip(filenames, predicted_labels):\n",
    "    print(f\"{fname} => {pred_label}\")\n",
    "\n",
    "# Optional: Save predictions and filenames to a CSV file\n",
    "df = pd.DataFrame({\"Filename\": filenames, \"Predicted Class\": predicted_labels})\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f01ed",
   "metadata": {},
   "source": [
    "The following script generates class-specific saliency maps for GC-IMS data using a CNN model.\n",
    "It loads a model and test dataset, computes SmoothGrad-enhanced saliency maps using tf-keras-vis, and visualizes them with a custom colormap. The resulting heatmaps are saved with appropriate scientific axes and labels, enabling interpretation of which regions most influence the model's classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333dc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Load the trained CNN model\n",
    "model_path = r\"Path to the trained CNN model\"\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Successfully loaded model from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {model_path}. Error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Get input shape and target size directly from the loaded model\n",
    "try:\n",
    "    # Assumes model input shape is (None, H, W, C)\n",
    "    model_input_shape = model.input_shape[1:]\n",
    "    target_size = (model_input_shape[0], model_input_shape[1])\n",
    "    print(f\"Model expects input shape: {model_input_shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not determine model input shape. Error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "data_dir = r\"Directory for .npy files\"\n",
    "\n",
    "# Create output directory for saving saliency maps\n",
    "output_dir = r\"Output directory for saving saliency maps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Saliency from tf-keras-vis\n",
    "saliency = Saliency(model)\n",
    "\n",
    "# Data loading\n",
    "data_path = Path(data_dir)\n",
    "if not data_path.exists():\n",
    "    print(f\"Error: Data directory not found at {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Find and sort all .npy files to ensure consistent order\n",
    "npy_files = sorted(list(data_path.glob(\"**/*.npy\")))\n",
    "if not npy_files:\n",
    "    print(f\"Error: No .npy files found in {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "class_names_set = set()\n",
    "for npy_file in npy_files:\n",
    "    class_name = npy_file.parent.name\n",
    "    class_names_set.add(class_name)\n",
    "\n",
    "sorted_class_names = sorted(list(class_names_set))\n",
    "class_indices = {name: i for i, name in enumerate(sorted_class_names)}\n",
    "print(f\"Found classes: {class_indices}\")\n",
    "\n",
    "\n",
    "# Generate saliency maps for all data in the dataset\n",
    "for npy_file in tqdm(npy_files, desc=\"Generating saliency maps\"):\n",
    "    fig = None\n",
    "    try:\n",
    "\n",
    "        # Get class_index from folder name\n",
    "        class_name = npy_file.parent.name\n",
    "        class_index = class_indices[class_name]\n",
    "\n",
    "        # Get filename\n",
    "        relative_path_str = str(npy_file.relative_to(data_path))\n",
    "        filename = (\n",
    "            relative_path_str.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(os.sep, \"_\")\n",
    "        )\n",
    "\n",
    "        # Load, resize, and scale data (same as training scripts)\n",
    "        data = np.load(npy_file)\n",
    "\n",
    "        data_tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "        data_expanded = tf.expand_dims(data_tensor, axis=-1)\n",
    "        data_resized_tensor = tf.image.resize(data_expanded, target_size)\n",
    "        data_resized_numpy = data_resized_tensor.numpy()\n",
    "\n",
    "        max_val = data_resized_numpy.max()\n",
    "        data_scaled = data_resized_numpy / max_val\n",
    "\n",
    "        X_val = np.expand_dims(data_scaled, axis=0)\n",
    "\n",
    "        # Define score function for the true class\n",
    "        score = CategoricalScore([class_index])\n",
    "\n",
    "        # Compute SmoothGrad-enhanced saliency map\n",
    "        saliency_map = saliency(score, X_val, smooth_samples=300, smooth_noise=0.05)[0]\n",
    "\n",
    "        # Custom colormap: white (low saliency) to red (high saliency)\n",
    "        white_to_red = LinearSegmentedColormap.from_list(\"white_red\", [\"white\", \"red\"])\n",
    "\n",
    "        # Apply cutoff threshold (optional)\n",
    "        cutoff = 0.0\n",
    "        saliency_map = np.maximum(saliency_map, cutoff)\n",
    "\n",
    "        # Plot saliency map with scientific axis formatting\n",
    "        fig, ax = plt.subplots(figsize=(4, 4), dpi=300)\n",
    "\n",
    "        im = ax.imshow(\n",
    "            saliency_map,\n",
    "            cmap=white_to_red,\n",
    "            vmin=cutoff,\n",
    "            vmax=saliency_map.max(),\n",
    "            extent=[1, 2.5, 50, 900],  # [dt_min, dt_max, rt_min, rt_max]\n",
    "            origin=\"upper\",\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Axis labeling and tick formatting\n",
    "        ax.set_xlabel(\"Drift Time\")\n",
    "        ax.set_ylabel(\"Retention Time\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "        ax.yaxis.set_minor_locator(ticker.MultipleLocator(20))\n",
    "        ax.tick_params(which=\"major\", length=6, labelsize=8)\n",
    "        ax.tick_params(which=\"minor\", length=3, labelsize=0)\n",
    "\n",
    "        # Colorbar with label\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\"Saliency Intensity\", fontsize=8)\n",
    "        cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "        # Save plot to output directory\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, f\"saliency_{filename}.png\"), bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate saliency map for {npy_file.name}. Error: {e}\")\n",
    "        if fig is not None and plt.fignum_exists(fig.number):\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96069914",
   "metadata": {},
   "source": [
    "The following script computes class-specific saliency maps for GC-IMS heatmap data using a trained CNN model. Instead of visualizing the saliency maps directly, it flattens each map into a 1D feature vector and groups them according to their predicted class. The resulting feature vectors are saved as NumPy arrays (.npy) per class, forming a structured representation of salient regions for each sample.\n",
    "\n",
    "Note:\n",
    "These saved matrices are used in the next section to perform PCA and generate a \"single representative image per class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Load the trained CNN model\n",
    "model_path = r\"Path to the trained CNN model\"\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Successfully loaded model from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {model_path}. Error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Get input shape and target size directly from the loaded model\n",
    "try:\n",
    "    # Assumes model input shape is (None, H, W, C)\n",
    "    model_input_shape = model.input_shape[1:]\n",
    "    target_size = (model_input_shape[0], model_input_shape[1])\n",
    "    print(f\"Model expects input shape: {model_input_shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not determine model input shape. Error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "data_dir = r\"Directory for .npy files\"\n",
    "\n",
    "# Prepare output directories for Feature matrices\n",
    "output_dir = r\"Output directory for saving feature matrices\"\n",
    "feature_output_dir = os.path.join(output_dir, \"Feature_Matrices\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(feature_output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Saliency from tf-keras-vis\n",
    "saliency = Saliency(model)\n",
    "\n",
    "# Data loading\n",
    "data_path = Path(data_dir)\n",
    "if not data_path.exists():\n",
    "    print(f\"Error: Data directory not found at {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Find and sort all .npy files to ensure consistent order\n",
    "npy_files = sorted(list(data_path.glob(\"**/*.npy\")))\n",
    "if not npy_files:\n",
    "    print(f\"Error: No .npy files found in {data_dir}\")\n",
    "    sys.exit()\n",
    "\n",
    "class_names_set = set()\n",
    "for npy_file in npy_files:\n",
    "    class_name = npy_file.parent.name\n",
    "    class_names_set.add(class_name)\n",
    "\n",
    "sorted_class_names = sorted(list(class_names_set))\n",
    "class_indices = {name: i for i, name in enumerate(sorted_class_names)}\n",
    "print(f\"Found classes: {class_indices}\")\n",
    "\n",
    "# Dictionary to collect flattened saliency vectors by class\n",
    "feature_matrices = {}\n",
    "\n",
    "# Generate saliency maps for all data in the dataset\n",
    "for npy_file in tqdm(npy_files, desc=\"Generating saliency maps\"):\n",
    "\n",
    "    # Get class_index from folder name\n",
    "    class_name = npy_file.parent.name\n",
    "    class_index = class_indices[class_name]\n",
    "\n",
    "    # Get filename\n",
    "    relative_path_str = str(npy_file.relative_to(data_path))\n",
    "    filename = (\n",
    "        relative_path_str.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(os.sep, \"_\")\n",
    "    )\n",
    "\n",
    "    # Load, resize, and scale data (same as training scripts)\n",
    "    data = np.load(npy_file)\n",
    "\n",
    "    data_tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "    data_expanded = tf.expand_dims(data_tensor, axis=-1)\n",
    "    data_resized_tensor = tf.image.resize(data_expanded, target_size)\n",
    "    data_resized_numpy = data_resized_tensor.numpy()\n",
    "\n",
    "    max_val = data_resized_numpy.max()\n",
    "    data_scaled = data_resized_numpy / max_val\n",
    "\n",
    "    X_val = np.expand_dims(data_scaled, axis=0)\n",
    "\n",
    "    # Define score function for the true class\n",
    "    score = CategoricalScore([class_index])\n",
    "\n",
    "    # Compute SmoothGrad-enhanced saliency map\n",
    "    saliency_map = saliency(score, X_val, smooth_samples=300, smooth_noise=0.05)[0]\n",
    "\n",
    "    # Apply cutoff threshold (optional)\n",
    "    cutoff = 0.0\n",
    "    saliency_map = np.maximum(saliency_map, cutoff)\n",
    "\n",
    "    # Flatten saliency map to 1D vector\n",
    "    feature_vector = saliency_map.flatten()\n",
    "\n",
    "    # Collect feature vector under corresponding class\n",
    "    if class_index not in feature_matrices:\n",
    "        feature_matrices[class_index] = []\n",
    "    feature_matrices[class_index].append(feature_vector)\n",
    "\n",
    "# Save the feature matrices per class\n",
    "for class_idx, vectors in feature_matrices.items():\n",
    "    matrix = np.stack(vectors)  # shape: [num_samples, num_features]\n",
    "    np.save(\n",
    "        os.path.join(feature_output_dir, f\"class_{class_idx}_saliency_matrix.npy\"),\n",
    "        matrix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc3a98",
   "metadata": {},
   "source": [
    "The following script performs PCA on the class-specific saliency matrices generated earlier.\n",
    "Each matrix contains flattened saliency maps for one class, where rows represent samples and columns represent pixel-wise features. PCA is applied to reduce dimensionality and extract the dominant patterns across all samples of a class.\n",
    "The first principal component (PC1), also known as the loading vector, is reshaped back to the original image dimensions\n",
    "and visualized as a heatmap. This representative image highlights the most influential and class-discriminative regions\n",
    "across all saliency maps for that class.\n",
    "\n",
    "Note:\n",
    "These loading images serve as a \"single representative visualization per class\", revealing which regions of the GC-IMS\n",
    "heatmaps are consistently important across all samples in that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cc07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Directory containing saved saliency feature matrices (one .npy file per class)\n",
    "feature_matrix_dir = r\"Input directory for feature matrices\"\n",
    "\n",
    "# Directory to store PCA loading images (output)\n",
    "pca_output_dir = r\"Directory to store PCA loading images output\"\n",
    "os.makedirs(pca_output_dir, exist_ok=True)\n",
    "\n",
    "# Original dimensions of the saliency maps\n",
    "height, width = 248, 200\n",
    "\n",
    "# Loop through all class-wise feature matrices\n",
    "for filename in os.listdir(feature_matrix_dir):\n",
    "    if filename.endswith(\".npy\") and filename.startswith(\"class_\"):\n",
    "        # Extract class index from filename\n",
    "        class_idx = filename.split(\"_\")[1]\n",
    "\n",
    "        # Load feature matrix: shape = (num_samples, height * width)\n",
    "        matrix = np.load(os.path.join(feature_matrix_dir, filename))\n",
    "\n",
    "        # Perform PCA on the feature matrix\n",
    "        pca = PCA()\n",
    "        pca.fit(matrix)\n",
    "\n",
    "        # Print explained variance ratio for reference (e.g., to verify PC1 contribution)\n",
    "        print(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Extract the first principal component (loading vector)\n",
    "        pc1 = pca.components_[0].reshape((height, width))  # Reshape to image format\n",
    "\n",
    "        # Normalize PC1 to [-1, 1] for visualization\n",
    "        max_abs = np.max(np.abs(pc1))\n",
    "        pc1_normalized = pc1 / max_abs\n",
    "\n",
    "        # Create figure for visualization\n",
    "        fig, ax = plt.subplots(figsize=(4, 4), dpi=300)\n",
    "\n",
    "        # Display the normalized PC1 as a heatmap using a blue-white-red colormap\n",
    "        im = ax.imshow(\n",
    "            pc1_normalized,\n",
    "            cmap=\"bwr\",\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            extent=[1, 2.5, 50, 900],  # Drift time (x) and Retention time (y) axes\n",
    "            origin=\"upper\",  # Maintain correct axis direction\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Axis labels\n",
    "        ax.set_xlabel(\"Drift time RIP relative (ms)\")\n",
    "        ax.set_ylabel(\"Retention Time (s)\")\n",
    "\n",
    "        # Set major and minor ticks for better interpretability\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "        ax.yaxis.set_minor_locator(ticker.MultipleLocator(20))\n",
    "\n",
    "        # Tweak tick appearance\n",
    "        ax.tick_params(which=\"major\", length=6, labelsize=8)\n",
    "        ax.tick_params(which=\"minor\", length=3, labelsize=0)\n",
    "\n",
    "        # Add colorbar to indicate saliency loading intensity\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\"Saliency Intensity\", fontsize=8)\n",
    "        cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "        # Save the PC1 loading image for the class\n",
    "        save_path = os.path.join(pca_output_dir, f\"class_{class_idx}_pc1_loading.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
